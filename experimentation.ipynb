{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Envioronment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv venv\n",
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install \"google-cloud-bigquery>=3.17\"\n",
    "!pip3 install \"google-cloud-aiplatform>=1.38\"\n",
    "!pip3 install \"pandas>=2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import generative_models\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "project_id = \"\"\n",
    "\n",
    "queued_jobs_bucket_name = f\"{project_id}-queued-jobs-test\"\n",
    "in_progress_jobs_bucket_name = f\"{project_id}-in-progress-jobs-test\"\n",
    "completed_jobs_bucket_name = f\"{project_id}-completed-jobs-test\"\n",
    "failed_jobs_bucket_name = f\"{project_id}-failed-jobs-test\"\n",
    "\n",
    "\n",
    "dataset_id = \"\"\n",
    "\n",
    "raw_target_table = \"\"\n",
    "target_table = \"\"\n",
    "\n",
    "raw_source_tables = [\"source-uipetmis\",\"source-uispet\"]\n",
    "raw_source_tables_wildcard = 'source*'\n",
    "source_table = \"\"\n",
    "\n",
    "output_table = \"\"\n",
    "\n",
    "# Initialise BQ client\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Source and target Schemas from BigQuery\n",
    "\n",
    "To reproduce yourself, use the .csvs in this directory and upload to BigQuery in your own Project. Do not use the provided spreadsheets directly as I have done some (minimal) pre-processing on the spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_ref_and_create_new_table(project_id, dataset_id, raw_table, new_table, new_col):\n",
    "    \"\"\"Adds 'Source_Unique_Ref' column if missing, then creates a new BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        project_id: BigQuery project ID.\n",
    "        dataset_id: BigQuery dataset ID.\n",
    "        raw_tables: Dict containing raw table names\n",
    "        new_table: Desired name of finalised table\n",
    "        new_col: the name of the reference column for the table\n",
    "    \"\"\"\n",
    "\n",
    "    raw_query = f\"\"\"\n",
    "        SELECT *  \n",
    "        FROM `{project_id}.{dataset_id}.{raw_table}`\n",
    "    \"\"\"\n",
    "    raw_df = client.query(raw_query).to_dataframe()\n",
    "    raw_df[new_col] = range(1, len(raw_df) + 1)  \n",
    "    new_table_id = f\"{project_id}.{dataset_id}.{new_table}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()  \n",
    "    job = client.load_table_from_dataframe(raw_df, new_table_id, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    return job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source table setup\n",
    "source_table_ref = client.dataset(dataset_id).table(source_table) \n",
    "try:\n",
    "    client.get_table(source_table_ref)  # Will raise NotFound if the table doesn't exist\n",
    "    print(\"Source table '{}' exists.\".format(source_table))\n",
    "except:\n",
    "    print(\"Source table '{}' does not exist.\".format(source_table))\n",
    "    print(\"creating Source table...\")\n",
    "    new_source_col = 'Source_Unique_Ref'\n",
    "    add_unique_ref_and_create_new_table(project_id, dataset_id, raw_source_tables_wildcard, source_table, new_source_col)\n",
    "\n",
    "source_query = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{project_id}.{dataset_id}.{source_table}`\n",
    "\"\"\"\n",
    "source_df = client.query(source_query).to_dataframe()\n",
    "print(f\"source_df length is {source_df.shape[0]}\")\n",
    "\n",
    "\n",
    "# Target table setup\n",
    "target_table_ref = client.dataset(dataset_id).table(target_table) \n",
    "try:\n",
    "    client.get_table(target_table_ref)  # Will raise NotFound if the table doesn't exist\n",
    "    print(f\"Target table {target_table} exists.\")\n",
    "except:\n",
    "    print(f\"Target table {target_table} does not exist.\")\n",
    "    print(\"creating Target table...\")\n",
    "    new_target_col = 'Target_Unique_Ref'   \n",
    "    add_unique_ref_and_create_new_table(project_id, dataset_id, raw_target_table, target_table, new_target_col)\n",
    "\n",
    "target_query = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{project_id}.{dataset_id}.{target_table}`\n",
    "\"\"\"\n",
    "target_df = client.query(target_query).to_dataframe()\n",
    "print(f\"source_df length is {target_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to group and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_string(df):\n",
    "    \"\"\"Converts a DataFrame to a string with column names and row values.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "        A string representation of the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    output = f\"Column Names: {', '.join(df.columns)}\\n\"  # Header with column names\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_string = ', '.join(str(value) for value in row)\n",
    "        output += f\"Row: {row_string}\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_function_call(function_call):\n",
    "    \"\"\"Parses a FunctionCall object, adds a description, and returns a JSON-compatible dictionary.\n",
    "\n",
    "    Args:\n",
    "        function_call: The FunctionCall object to parse.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the function name, attributes, and description.\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\n",
    "        \"function_name\": function_call.name,\n",
    "        \"attributes\": {},\n",
    "    }\n",
    "    for key, value in function_call.args.items():\n",
    "        result[\"attributes\"][key] = value\n",
    "\n",
    "    return result\n",
    "\n",
    "def convert_dict_to_list_of_dicts(dict):\n",
    "    \"\"\"Converts a dictionary of lists and strings to a list of flat dictionaries.\n",
    "\n",
    "    Args:\n",
    "        data: The input dictionary containing lists and strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a  \n",
    "        combination of elements from the input lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_attribute_dicts = []\n",
    "    string_keys = []\n",
    "    list_keys = []\n",
    "\n",
    "    for key, value in dict.items() :\n",
    "        if isinstance(value, str):\n",
    "            string_keys.append(key)\n",
    "        else:\n",
    "            list_keys.append(key)       \n",
    "    \n",
    "    for i in range (len(dict[list_keys[0]])):\n",
    "        new_dict = {}\n",
    "        \n",
    "        for key in list_keys:\n",
    "            new_dict[key] = dict[key][i]\n",
    "        for key in string_keys:\n",
    "            new_dict[key] = dict[key]\n",
    "\n",
    "        list_of_attribute_dicts.append(new_dict)\n",
    "\n",
    "    return list_of_attribute_dicts\n",
    "\n",
    "def create_df_from_target_row_df_and_list_of_dicts(list_of_attribute_dicts, test_target_df_row):\n",
    "    \"\"\"\n",
    "    Appends rows to a DataFrame, combining a base row with data from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        list_of_dicts: A list of dictionaries, each representing column values.\n",
    "        test_target_df_row: A DataFrame row containing base columns.\n",
    "\n",
    "    Returns:\n",
    "        The modified DataFrame with the newly appended rows.\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "     \n",
    "    for attribute_dict in list_of_attribute_dicts:\n",
    "        df = test_target_df_row.copy()  # Make a copy \n",
    "        for key in attribute_dict.keys(): # Combine the base row with the current dictionary\n",
    "            df[key] = attribute_dict[key]\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def merge_dataframes_and_string(target_df_row, source_df_row, confidence_level):\n",
    "    target_df_row = target_df_row.reset_index(drop=True)\n",
    "    source_df_row = source_df_row.reset_index(drop=True)\n",
    "    merged_df = pd.concat([target_df_row, source_df_row], axis=1)\n",
    "    confidence_df = pd.DataFrame({'Confidence_Levels': [confidence_level]})\n",
    "    final_df = pd.concat([merged_df, confidence_df], axis=1)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the subdivided groups for the source and destination schemas\n",
    "\n",
    "This is required so we can come in below the maximum token size for Gemini\n",
    "(32K input, 2K output) https://ai.google.dev/models/gemini#model_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_grouping_levels = ['Tranche', 'Level_1', 'Level_2', 'Level_3', 'Level_4']\n",
    "# target_df_groups = create_df_groups(target_df,target_grouping_levels)\n",
    "\n",
    "# target_string_groups = []\n",
    "# for path, target_df_group in target_df_groups.items():\n",
    "#     target_string = dataframe_to_string(target_df_group)\n",
    "#     target_string_groups.append(target_string)\n",
    "    \n",
    "# print(f\"Number of target schema dataframe groupings: {len(target_df_groups)}\")\n",
    "# print(f\"Number of target schema string groupings: {len(target_string_groups)}\\n\")\n",
    "# print(\"*************\\n\")\n",
    "\n",
    "# source_grouping_levels = ['SchemaName', 'TableName']\n",
    "# source_df_groups = create_df_groups(source_df, source_grouping_levels)\n",
    "\n",
    "# source_string_groups = []\n",
    "# for path, source_group_df in source_df_groups.items():\n",
    "#     source_group_sting = dataframe_to_string(source_group_df)\n",
    "#     source_string_groups.append(source_group_sting)\n",
    "    \n",
    "# print(f\"Number of source schema dataframe groupings: {len(source_df_groups)}\")\n",
    "# print(f\"Number of source schema string groupings: {len(source_string_groups)}\\n\")\n",
    "\n",
    "# #Further split up the source_df_groups to make sure there is no group larger than maximum_fields_per_request variable. This prevents LLM innacuacies when the number of requested field mappings is too high.\n",
    "# maximum_fields_per_request = 12\n",
    "\n",
    "# target_row_number = 42\n",
    "\n",
    "# chopped_source_df_groups = chop_source_df_groups(source_df_groups, maximum_fields_per_request)\n",
    "# print(f\"Row {target_row_number}: Number of chopped source schema dataframe groupings: {len(chopped_source_df_groups)}\")\n",
    "\n",
    "# chopped_length_counts = {}\n",
    "# for group_df in chopped_source_df_groups.values():\n",
    "#     group_length = len(group_df)\n",
    "#     if group_length in chopped_length_counts:\n",
    "#         chopped_length_counts[group_length] += 1\n",
    "#     else:\n",
    "#         chopped_length_counts[group_length] = 1\n",
    "# print(f\"Row {target_row_number}: Distribution of chopped lengths:\")\n",
    "# for length, count in chopped_length_counts.items():\n",
    "#     print(f\"{count} x groups with length {length}\")\n",
    "\n",
    "# merged_source_df_groups = merge_source_df_groups(chopped_source_df_groups, maximum_fields_per_request)\n",
    "# print(f\"Row {target_row_number}: Number of merged source schema dataframe groupings: {len(merged_source_df_groups)}\")\n",
    "\n",
    "# merged_length_counts = {}\n",
    "# for group_df in merged_source_df_groups.values():\n",
    "#     group_length = len(group_df)\n",
    "#     if group_length in merged_length_counts:\n",
    "#         merged_length_counts[group_length] += 1\n",
    "#     else:\n",
    "#         merged_length_counts[group_length] = 1\n",
    "# print(f\"Row {target_row_number}: Distribution of merged lengths:\")\n",
    "# for length, count in merged_length_counts.items():\n",
    "#     print(f\"{count} x groups with length {length}\")\n",
    "\n",
    "# combined_source_df_groups = merge_small_groups(merged_source_df_groups)\n",
    "# print(f\"Row {target_row_number}: Number of combined source schema dataframe groupings: {len(combined_source_df_groups)}\")\n",
    "\n",
    "# combined_length_counts = {}\n",
    "# for group_df in combined_source_df_groups.values():\n",
    "#     group_length = len(group_df)\n",
    "#     if group_length in combined_length_counts:\n",
    "#         combined_length_counts[group_length] += 1\n",
    "#     else:\n",
    "#         combined_length_counts[group_length] = 1\n",
    "# print(f\"Row {target_row_number}: Distribution of combined lengths:\")\n",
    "# for length, count in combined_length_counts.items():\n",
    "#     print(f\"{count} x groups with length {length}\")\n",
    "\n",
    "# unmapped_source_string_groups = []\n",
    "# for path, merged_source_df_group in merged_source_df_groups.items():\n",
    "#     merged_source_string_group = dataframe_to_string(merged_source_df_group)\n",
    "#     unmapped_source_string_groups.append(merged_source_string_group)\n",
    "# print(f\"Number of merged source schema string groupings: {len(unmapped_source_string_groups)}\\n\")\n",
    "\n",
    "# # random_number_in_source_range = random.randint(0, len(unmapped_source_string_groups)-1)\n",
    "# # print(f\"showing randomly chosen source string group {random_number_in_source_range}:\")\n",
    "# # random_source_string_group = unmapped_source_string_groups[random_number_in_source_range]\n",
    "# # print(random_source_string_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over all source fields groups (~500), for a single target field\n",
    "\n",
    "- First we select a single Target schema field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "objectId = \"target-row-100-source-groups-0-52\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "queued_jobs_bucket = storage_client.bucket(queued_jobs_bucket_name)\n",
    "failed_jobs_bucket = storage_client.bucket(failed_jobs_bucket_name)\n",
    "blob = queued_jobs_bucket.blob(objectId)\n",
    "\n",
    "if not blob.exists():\n",
    "    print(f\"File '{objectId}' not found in bucket '{queued_jobs_bucket_name}'. Job already picked from queue. Container instance completing with 204 message\")\n",
    "\n",
    "pattern = r\"^target-row-(\\d{1,4})-source-groups-(\\d{1,4})-(\\d{1,4})$\"\n",
    "match = re.match(pattern, objectId)\n",
    "if not match:\n",
    "    msg = \"objectId is not in the expected format: ^target-row-(\\d{3})-source-groups-(\\d{3})-(\\d{3})$\"\n",
    "    print(msg)\n",
    "\n",
    "target_row = int(match.group(1))\n",
    "source_group_start = int(match.group(2))\n",
    "source_group_end = int(match.group(3))\n",
    "\n",
    "print(f\"target_row {target_row} source_group_start {source_group_start} source_group_end {source_group_end}\")\n",
    "\n",
    "# Prepare target field\n",
    "target_df_row = target_df.iloc[[target_row]]\n",
    "target_string_row = dataframe_to_string(target_df_row)\n",
    "print(\"\\ntarget_string_row\")\n",
    "print(target_string_row)\n",
    "\n",
    "\n",
    "contents = blob.download_as_string().decode('utf-8')\n",
    "unmapped_source_string_groups = []\n",
    "unmapped_source_string_groups = contents.split(\"\\n\\n\")  # Split by double newlines\n",
    "\n",
    "print(\"\\nunmapped_source_string_groups[0]\")\n",
    "print(unmapped_source_string_groups[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we create the FunctionDeclaration. This helps us get a more structured and consistent output from the LLM which is helpful for use cases such as this when we are dealing with structured data. See [Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling)\n",
    "- Then we prepare the prompt to send to Gemini. The prompt is intentionally very verbose and repetitive, as well as directly refering to the declared function. Further optimisations and improvements could be made if spent tuning the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "set_source_field_mapping_confidence_levels = generative_models.FunctionDeclaration(\n",
    "    name=\"set_source_field_mapping_confidence_levels\",\n",
    "    description=\"\"\"Sets the mapping confidence values for each source field for a given target field.\n",
    "\n",
    "Here is a general example to help you understand how to use the set_source_field_mapping_confidences_tool correctly. This is only an example to show the source and target field structures.:\n",
    "\n",
    "Assuming you had previously decided on the following mapping confidence levels (but it is important that you come up with your own values for mapping condifence level rather than specifically using these values):\n",
    "a mapping confidence level of 2 for the field with Source_Unique_Ref=158\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=159\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=1290\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=579\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=638\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=970\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=3317\n",
    "a mapping confidence level of 3 for the field with Source_Unique_Ref=160\n",
    "a mapping confidence level of 1 for the field with Source_Unique_Ref=1910\n",
    "a mapping confidence level of 5 for the field with Source_Unique_Ref=2280\n",
    "\n",
    "Then this function would be used to set the mapping confidence levels for each of the source fields, where your input parameter source_field_mapping_confidences would be:\n",
    "source_field_mapping_confidences = [\n",
    "    {'Source_Unique_Ref':158,'mapping_confidence_level':'2'},\n",
    "    {'Source_Unique_Ref':159,'mapping_confidence_level':'2'},\n",
    "    {'Source_Unique_Ref':1290,'mapping_confidence_level':'1'},\n",
    "    {'Source_Unique_Ref':579,'mapping_confidence_level':'1'},\n",
    "    {'Source_Unique_Ref':638,'mapping_confidence_level':'1'},\n",
    "    {'Source_Unique_Ref':970,'mapping_confidence_level':'1'},\n",
    "    {'Source_Unique_Ref':3317,'mapping_confidence_level':'1'},\n",
    "    {'Source_Unique_Ref':160,'mapping_confidence_level':'3'},\n",
    "    {'Source_Unique_Ref':1910,'mapping_confidence_level':'1'},\n",
    "    {'Source_Unique_Ref':2280,'mapping_confidence_level':'5'}\n",
    "]\"\"\",\n",
    "\n",
    "\n",
    "# Then this function would be used to set the mapping confidence levels for each of the source fields, where your input parameters would be\n",
    "# 'mapping_confidence_level'=[\"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"1\", \"5\"], 'Source_Unique_Ref': [158, 159, 1290, 579, 638, 970, 3317, 160, 1910, 2280]\n",
    "# And notice that the array index positions for each parameter align with each other to represent the mapping for a particular source field. This is very important.\"\"\",\n",
    "    # parameters={\n",
    "    #     \"type\": \"object\",\n",
    "    #     \"properties\": {\n",
    "    #         \"Source_Unique_Ref\": {\n",
    "    #             \"type\": \"array\",\n",
    "    #             \"description\": \"An array containing each of the Source_Unique_Ref values for the set of source fields to set a mapping confidence level for.\",\n",
    "    #             \"items\" : {\n",
    "    #                 \"type\": \"integer\"\n",
    "    #             },\n",
    "    #             \"example\": [158, 159, 1290, 579, 638, 970, 3317, 160, 1910, 2280]\n",
    "    #         },\n",
    "    #         \"mapping_confidence_level\": {\n",
    "    #             \"type\": \"array\",\n",
    "    #             \"description\": \"The mapping confidence level for the corresponding source field in the same index in the Source_Unique_Ref parameter. It is very important that the array indexes for mapping_confidence_level align to the Source_Unique_Ref so the mapping confidence levels are aligned to the correct source fields.\",\n",
    "    #             \"items\" : {\n",
    "    #                 \"type\": \"string\"\n",
    "    #             },\n",
    "    #             \"example\": [\"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"1\", \"5\"]\n",
    "    #         },\n",
    "    #     },\n",
    "    #     \"required\": [\"Source_Unique_Ref\", \"mapping_confidence_level\"]\n",
    "    # },\n",
    "\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"source_field_mapping_confidences\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"A List of objects where each object in the list contains the source field's Source_Unique_Ref and the mapping_confidence_level for that source field.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"Source_Unique_Ref\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The reference ID for the source field.\"\n",
    "                        },\n",
    "                        \"mapping_confidence_level\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\n",
    "                            \"description\": \"The confidence level for the mapping (an integer between 1 and 5).\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"Source_Unique_Ref\", \"mapping_confidence_level\"]\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"source_field_mapping_confidences\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "set_source_field_mapping_confidence_levels_tool = generative_models.Tool(\n",
    "    function_declarations=[set_source_field_mapping_confidence_levels]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_unmapped_source_string_group = unmapped_source_string_groups[0]\n",
    "field_count = single_unmapped_source_string_group.count('Row:')\n",
    "\n",
    "print(\"************************************\")\n",
    "print(f\"{field_count} source fields in group\")\n",
    "print(single_unmapped_source_string_group)\n",
    "print(\"mapping to\")\n",
    "print(target_string_row)\n",
    "\n",
    "prompt = f\"\"\"You are Data Engineer working for an insurance company. As part of a data migration project you need to assist with mapping fields in a source data schema fields in a target data schema.\n",
    "The source and destination schemas are both complex and nested.\n",
    "You will be shown 1 field in the target schema and multiple fields in the source schema.\n",
    "The mappings will not be exactly one to one: Instead of providing a one-to-one mapping for a single source schema to a single destiation schema, your job is to provide a mapping confidence level for how well you think each of the fields for the source schemas you see will map to the field for the target schema.\n",
    "\n",
    "The field from the target schema is described here:\n",
    "{target_string_row}\n",
    "\n",
    "The fields taken from the source schema are described here:\n",
    "{single_unmapped_source_string_group}\n",
    "\n",
    "Based on your knowledge of the insurance industry, pets, pet insurance, you will provide a mapping confidence level for how well each of the source fields map to the target field.\n",
    "The confidence level is a number between 1 and 5 where:\n",
    "1 means there is a very very small chance that the fields could be a match\n",
    "2 means there is a small chance that the fields colud be a match\n",
    "3 means there is a medium chance that the fields could be a match\n",
    "4 means there is a good chance that the fields could be a match\n",
    "5 means there is a very good chance that the fields could be a match\n",
    "\n",
    "You should decide on a mapping confidence level for each of the source fields, then set the mapping confidence level for each field using and use the value for Source_Unique_Ref for each source field to reference it with its corresponding mapping confidence level.\n",
    "Then YOU MUST USE the available function set_source_field_mapping_confidence_levels in the set_source_field_mapping_confidence_levels_tool to set your mappings confidence level for each of the source fields.\n",
    "YOU MUST USE THIS FUNCTION.\"\"\"\n",
    "\n",
    "model_response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[set_source_field_mapping_confidence_levels_tool],\n",
    ")\n",
    "\n",
    "if not model_response.candidates[0].content.parts[0].function_call:\n",
    "    print(\"did not use fn call! retrying with a more explicit prompt\")\n",
    "    prompt += \"\"\"\n",
    "YOU MUST USE THIS FUNCTION.\"\"\"\n",
    "    model_response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"temperature\": 0},\n",
    "        tools=[set_source_field_mapping_confidence_levels_tool],\n",
    "    )\n",
    "\n",
    "function_call_json = parse_function_call(model_response.candidates[0].content.parts[0].function_call)\n",
    "attributes_dict = function_call_json[\"attributes\"]\n",
    "print(f\"Received mapping response from Gemini: {attributes_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proto.marshal.collections import repeated\n",
    "from proto.marshal.collections import maps\n",
    "\n",
    "def recurse_proto_repeated_composite(repeated_object):\n",
    "    repeated_list = []\n",
    "    for item in repeated_object:\n",
    "        if isinstance(item, repeated.RepeatedComposite):\n",
    "            item = recurse_proto_repeated_composite(item)\n",
    "            repeated_list.append(item)\n",
    "        elif isinstance(item, maps.MapComposite):\n",
    "            item = recurse_proto_marshal_to_dict(item)\n",
    "            repeated_list.append(item)\n",
    "        else:\n",
    "            repeated_list.append(item)\n",
    "\n",
    "    return repeated_list\n",
    "\n",
    "def recurse_proto_marshal_to_dict(marshal_object):\n",
    "    new_dict = {}\n",
    "    for k, v in marshal_object.items():\n",
    "      if not v:\n",
    "        continue\n",
    "      elif isinstance(v, maps.MapComposite):\n",
    "          v = recurse_proto_marshal_to_dict(v)\n",
    "      elif isinstance(v, repeated.RepeatedComposite):\n",
    "          v = recurse_proto_repeated_composite(v)\n",
    "      new_dict[k] = v\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in attributes_dict['source_field_mapping_confidences']:\n",
    "    print(recurse_proto_marshal_to_dict(attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_for_bq_upload = []\n",
    "\n",
    "for l, mapping_confidence in enumerate(attributes_dict['source_field_mapping_confidences']):\n",
    "    mapping_confidence_dict = recurse_proto_marshal_to_dict(mapping_confidence)\n",
    "    Source_Unique_Ref = mapping_confidence_dict['Source_Unique_Ref']\n",
    "    mapping_confidence_level_int = int(mapping_confidence_dict['mapping_confidence_level'])\n",
    "\n",
    "    source_df_row = source_df[source_df['Source_Unique_Ref']==Source_Unique_Ref]\n",
    "    \n",
    "    mapping_output = merge_dataframes_and_string(target_df_row, source_df_row,mapping_confidence_level_int)\n",
    "    df_list_for_bq_upload.append(mapping_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_for_bq_upload[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prepared the mapping as a df ready for upload to bigquery\")\n",
    "print(f\"df_list_for_bq_upload contains {len(df_list_for_bq_upload)} dataframes. Concatenating...\")\n",
    "df_for_bq_upload_concat = pd.concat(df_list_for_bq_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_bq_upload_concat.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(output_table)\n",
    "\n",
    "print(f\"Loading group_mapping into bigquery...\")\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[],\n",
    "    write_disposition=\"WRITE_APPEND\",\n",
    "    schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df_for_bq_upload_concat, table_ref, job_config=job_config)  \n",
    "job.result()  # Wait for job completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
