{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Envioronment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv venv\n",
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install \"google-cloud-bigquery>=3.17\"\n",
    "!pip3 install \"google-cloud-aiplatform>=1.38\"\n",
    "!pip3 install \"pandas>=2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import generative_models\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "project_id = \"\"\n",
    "dataset_id = \"\"\n",
    "\n",
    "raw_target_table = \"\"\n",
    "target_table = \"\"\n",
    "\n",
    "raw_source_tables = [\"source-uipetmis\",\"source-uispet\"]\n",
    "raw_source_tables_wildcard = 'source*'\n",
    "source_table = \"\"\n",
    "\n",
    "# Initialise BQ client\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Source and target Schemas from BigQuery\n",
    "\n",
    "To reproduce yourself, use the .csvs in this directory and upload to BigQuery in your own Project. Do not use the provided spreadsheets directly as I have done some (minimal) pre-processing on the spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_ref_and_create_new_table(project_id, dataset_id, raw_table, new_table, new_col):\n",
    "    \"\"\"Adds 'Source_Unique_Ref' column if missing, then creates a new BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        project_id: BigQuery project ID.\n",
    "        dataset_id: BigQuery dataset ID.\n",
    "        raw_tables: Dict containing raw table names\n",
    "        new_table: Desired name of finalised table\n",
    "        new_col: the name of the reference column for the table\n",
    "    \"\"\"\n",
    "\n",
    "    raw_query = f\"\"\"\n",
    "        SELECT *  \n",
    "        FROM `{project_id}.{dataset_id}.{raw_table}`\n",
    "    \"\"\"\n",
    "    raw_df = client.query(raw_query).to_dataframe()\n",
    "    raw_df[new_col] = range(1, len(raw_df) + 1)  \n",
    "    new_table_id = f\"{project_id}.{dataset_id}.{new_table}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()  \n",
    "    job = client.load_table_from_dataframe(raw_df, new_table_id, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    return job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source table setup\n",
    "source_table_ref = client.dataset(dataset_id).table(source_table) \n",
    "try:\n",
    "    client.get_table(source_table_ref)  # Will raise NotFound if the table doesn't exist\n",
    "    print(\"Source table '{}' exists.\".format(source_table))\n",
    "except:\n",
    "    print(\"Source table '{}' does not exist.\".format(source_table))\n",
    "    print(\"creating Source table...\")\n",
    "    new_source_col = 'Source_Unique_Ref'\n",
    "    add_unique_ref_and_create_new_table(project_id, dataset_id, raw_source_tables_wildcard, source_table, new_source_col)\n",
    "\n",
    "source_query = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{project_id}.{dataset_id}.{source_table}`\n",
    "\"\"\"\n",
    "source_df = client.query(source_query).to_dataframe()\n",
    "print(f\"source_df length is {source_df.shape[0]}\")\n",
    "\n",
    "\n",
    "# Target table setup\n",
    "target_table_ref = client.dataset(dataset_id).table(target_table) \n",
    "try:\n",
    "    client.get_table(target_table_ref)  # Will raise NotFound if the table doesn't exist\n",
    "    print(f\"Target table {target_table} exists.\")\n",
    "except:\n",
    "    print(f\"Target table {target_table} does not exist.\")\n",
    "    print(\"creating Target table...\")\n",
    "    new_target_col = 'Target_Unique_Ref'   \n",
    "    add_unique_ref_and_create_new_table(project_id, dataset_id, raw_target_table, target_table, new_target_col)\n",
    "\n",
    "target_query = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{project_id}.{dataset_id}.{target_table}`\n",
    "\"\"\"\n",
    "target_df = client.query(target_query).to_dataframe()\n",
    "print(f\"source_df length is {target_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to group and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_target_df_groups(df, grouping_levels):\n",
    "#     \"\"\"Groups a DataFrame by nested schema paths up to a specified level.\n",
    "\n",
    "#     Args:\n",
    "#         df: The DataFrame to group.\n",
    "#         grouping_levels: A List of strings specifying which columns should be included in the grouping.\n",
    "\n",
    "#     Returns:\n",
    "#         A dictionary of DataFrames, where keys are the nested paths, and\n",
    "#         values are DataFrames containing fields sharing that path. \n",
    "#     \"\"\"\n",
    "\n",
    "#     grouped_dfs = {}\n",
    "#     # levels = ['Tranche'] + [f'Level_{i}' for i in range(1, 4)] \n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         path = '.'.join(row[col] for col in grouping_levels if row[col] != 'n/a')\n",
    "#         if path not in grouped_dfs:\n",
    "#             grouped_dfs[path] = pd.DataFrame(columns=df.columns)  \n",
    "#         grouped_dfs[path] = pd.concat([grouped_dfs[path], row.to_frame().T], ignore_index=True)\n",
    "\n",
    "#     return grouped_dfs\n",
    "\n",
    "def create_df_groups(df, grouping_levels):\n",
    "    \"\"\"Groups a DataFrame by nested schema paths up to a specified level.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to group.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of DataFrames, where keys are the nested paths, and\n",
    "        values are DataFrames containing fields sharing that path. \n",
    "    \"\"\"\n",
    "\n",
    "    grouped_dfs = {}\n",
    "    # levels = ['SchemaName', 'TableName'] \n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = '.'.join(row[col] for col in grouping_levels)\n",
    "        if path not in grouped_dfs:\n",
    "            grouped_dfs[path] = pd.DataFrame(columns=df.columns)  \n",
    "        grouped_dfs[path] = pd.concat([grouped_dfs[path], row.to_frame().T], ignore_index=True)\n",
    "\n",
    "    return grouped_dfs\n",
    "\n",
    "def dataframe_to_string(df):\n",
    "    \"\"\"Converts a DataFrame to a string with column names and row values.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "        A string representation of the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    output = f\"Column Names: {', '.join(df.columns)}\\n\"  # Header with column names\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_string = ', '.join(str(value) for value in row)\n",
    "        output += f\"Row: {row_string}\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def chop_source_df_groups(source_df_groups, max_rows_per_group):\n",
    "    \"\"\"Chops source dataframe groups into smaller groups with a specified max number of rows.\n",
    "\n",
    "    Args:\n",
    "        source_df_groups: The dictionary of source dataframe groups.\n",
    "        max_rows_per_group: The maximum number of rows allowed in each group.\n",
    "\n",
    "    Returns:\n",
    "        A modified dictionary of source dataframe groups with smaller groups.\n",
    "    \"\"\"\n",
    "    chopped_source_df_groups = {}\n",
    "\n",
    "    for path, source_group_df in source_df_groups.items():\n",
    "        # Check if the group needs to be chopped\n",
    "        if len(source_group_df) <= max_rows_per_group:\n",
    "            chopped_source_df_groups[path] = source_group_df\n",
    "        else:\n",
    "            # Split the group into smaller groups with a maximum of max_rows_per_group rows\n",
    "            num_subgroups = len(source_group_df) // max_rows_per_group\n",
    "            remainder = len(source_group_df) % max_rows_per_group\n",
    "\n",
    "            for i in range(num_subgroups):\n",
    "                start_idx = i * max_rows_per_group\n",
    "                end_idx = (i + 1) * max_rows_per_group\n",
    "                sub_df = source_group_df.iloc[start_idx:end_idx]\n",
    "                chopped_source_df_groups[f\"{path}_subgroup_{i+1}\"] = sub_df\n",
    "\n",
    "            # Add the remainder as a separate subgroup\n",
    "            if remainder > 0:\n",
    "                sub_df = source_group_df.iloc[-remainder:]\n",
    "                chopped_source_df_groups[f\"{path}_subgroup_{num_subgroups+1}\"] = sub_df\n",
    "\n",
    "    return chopped_source_df_groups\n",
    "\n",
    "\n",
    "def merge_source_df_groups(source_df_groups, max_rows_per_group):\n",
    "    \"\"\"Chops source dataframe groups and combines smaller groups.\n",
    "\n",
    "    Args:\n",
    "        source_df_groups: The dictionary of source dataframe groups.\n",
    "        max_rows_per_group: The maximum number of rows allowed in each group.\n",
    "\n",
    "    Returns:\n",
    "        A modified dictionary of source dataframe groups with optimized sizing.\n",
    "    \"\"\"\n",
    "    chopped_source_df_groups = {}\n",
    "    group_paths = list(source_df_groups.keys())  # Get a list of group paths for iteration\n",
    "\n",
    "    i = 0\n",
    "    while i < len(group_paths):\n",
    "        current_path = group_paths[i]\n",
    "        current_group_df = source_df_groups[current_path]\n",
    "            \n",
    "        # Combine with subsequent groups while possible\n",
    "        while i + 1 < len(group_paths) and len(current_group_df) + len(source_df_groups[group_paths[i + 1]]) <= max_rows_per_group:\n",
    "            next_path = group_paths[i + 1]\n",
    "            next_group_df = source_df_groups[next_path]\n",
    "            current_group_df = pd.concat([current_group_df, next_group_df], ignore_index=True)\n",
    "            del source_df_groups[next_path]  # Remove the merged group\n",
    "            group_paths.pop(i + 1)  # Update the list of group paths\n",
    "\n",
    "        # Add the combined (or original) group\n",
    "        chopped_source_df_groups[current_path] = current_group_df\n",
    "        i += 1\n",
    "\n",
    "    return chopped_source_df_groups\n",
    "\n",
    "def merge_small_groups(chopped_source_df_groups):\n",
    "    \"\"\"Merges small groups (length <= 3) with their preceding groups.\n",
    "\n",
    "    Args:\n",
    "        chopped_source_df_groups: The dictionary of chopped dataframe groups.\n",
    "\n",
    "    Returns:\n",
    "        A modified dictionary of dataframe groups with fewer small groups.\n",
    "    \"\"\"\n",
    "    group_paths = list(chopped_source_df_groups.keys())\n",
    "    i = 1  # Start from the second group\n",
    "    while i < len(group_paths):\n",
    "        current_path = group_paths[i]\n",
    "        current_group_df = chopped_source_df_groups[current_path]\n",
    "\n",
    "        if len(current_group_df) <= 3:\n",
    "            prev_path = group_paths[i - 1]\n",
    "            prev_group_df = chopped_source_df_groups[prev_path]\n",
    "\n",
    "            # Merge with the previous group\n",
    "            chopped_source_df_groups[prev_path] = pd.concat([prev_group_df, current_group_df], ignore_index=True)\n",
    "\n",
    "            # Remove the current group\n",
    "            del chopped_source_df_groups[current_path]\n",
    "            group_paths.pop(i) \n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return chopped_source_df_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the subdivided groups for the source and destination schemas\n",
    "\n",
    "This is required so we can come in below the maximum token size for Gemini\n",
    "(32K input, 2K output) https://ai.google.dev/models/gemini#model_variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_grouping_levels = ['Tranche', 'Level_1', 'Level_2', 'Level_3', 'Level_4']\n",
    "target_df_groups = create_df_groups(target_df,target_grouping_levels)\n",
    "\n",
    "target_string_groups = []\n",
    "for path, target_df_group in target_df_groups.items():\n",
    "    target_string = dataframe_to_string(target_df_group)\n",
    "    target_string_groups.append(target_string)\n",
    "    \n",
    "print(f\"Number of target schema dataframe groupings: {len(target_df_groups)}\")\n",
    "print(f\"Number of target schema string groupings: {len(target_string_groups)}\\n\")\n",
    "print(\"*************\\n\")\n",
    "\n",
    "source_grouping_levels = ['SchemaName', 'TableName']\n",
    "source_df_groups = create_df_groups(source_df, source_grouping_levels)\n",
    "\n",
    "source_string_groups = []\n",
    "for path, source_group_df in source_df_groups.items():\n",
    "    source_group_sting = dataframe_to_string(source_group_df)\n",
    "    source_string_groups.append(source_group_sting)\n",
    "    \n",
    "print(f\"Number of source schema dataframe groupings: {len(source_df_groups)}\")\n",
    "print(f\"Number of source schema string groupings: {len(source_string_groups)}\\n\")\n",
    "\n",
    "#Further split up the source_df_groups to make sure there is no group larger than maximum_fields_per_request variable. This prevents LLM innacuacies when the number of requested field mappings is too high.\n",
    "maximum_fields_per_request = 12\n",
    "\n",
    "target_row_number = 42\n",
    "\n",
    "chopped_source_df_groups = chop_source_df_groups(source_df_groups, maximum_fields_per_request)\n",
    "print(f\"Row {target_row_number}: Number of chopped source schema dataframe groupings: {len(chopped_source_df_groups)}\")\n",
    "\n",
    "chopped_length_counts = {}\n",
    "for group_df in chopped_source_df_groups.values():\n",
    "    group_length = len(group_df)\n",
    "    if group_length in chopped_length_counts:\n",
    "        chopped_length_counts[group_length] += 1\n",
    "    else:\n",
    "        chopped_length_counts[group_length] = 1\n",
    "print(f\"Row {target_row_number}: Distribution of chopped lengths:\")\n",
    "for length, count in chopped_length_counts.items():\n",
    "    print(f\"{count} x groups with length {length}\")\n",
    "\n",
    "merged_source_df_groups = merge_source_df_groups(chopped_source_df_groups, maximum_fields_per_request)\n",
    "print(f\"Row {target_row_number}: Number of merged source schema dataframe groupings: {len(merged_source_df_groups)}\")\n",
    "\n",
    "merged_length_counts = {}\n",
    "for group_df in merged_source_df_groups.values():\n",
    "    group_length = len(group_df)\n",
    "    if group_length in merged_length_counts:\n",
    "        merged_length_counts[group_length] += 1\n",
    "    else:\n",
    "        merged_length_counts[group_length] = 1\n",
    "print(f\"Row {target_row_number}: Distribution of merged lengths:\")\n",
    "for length, count in merged_length_counts.items():\n",
    "    print(f\"{count} x groups with length {length}\")\n",
    "\n",
    "combined_source_df_groups = merge_small_groups(merged_source_df_groups)\n",
    "print(f\"Row {target_row_number}: Number of combined source schema dataframe groupings: {len(combined_source_df_groups)}\")\n",
    "\n",
    "combined_length_counts = {}\n",
    "for group_df in combined_source_df_groups.values():\n",
    "    group_length = len(group_df)\n",
    "    if group_length in combined_length_counts:\n",
    "        combined_length_counts[group_length] += 1\n",
    "    else:\n",
    "        combined_length_counts[group_length] = 1\n",
    "print(f\"Row {target_row_number}: Distribution of combined lengths:\")\n",
    "for length, count in combined_length_counts.items():\n",
    "    print(f\"{count} x groups with length {length}\")\n",
    "\n",
    "unmapped_source_string_groups = []\n",
    "for path, merged_source_df_group in merged_source_df_groups.items():\n",
    "    merged_source_string_group = dataframe_to_string(merged_source_df_group)\n",
    "    unmapped_source_string_groups.append(merged_source_string_group)\n",
    "print(f\"Number of merged source schema string groupings: {len(unmapped_source_string_groups)}\\n\")\n",
    "\n",
    "# random_number_in_source_range = random.randint(0, len(unmapped_source_string_groups)-1)\n",
    "# print(f\"showing randomly chosen source string group {random_number_in_source_range}:\")\n",
    "# random_source_string_group = unmapped_source_string_groups[random_number_in_source_range]\n",
    "# print(random_source_string_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we create the FunctionDeclaration. This helps us get a more structured and consistent output from the LLM which is helpful for use cases such as this when we are dealing with structured data. See [Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling)\n",
    "- Then we prepare the prompt to send to Gemini. The prompt is intentionally very verbose and repetitive, as well as directly refering to the declared function. Further optimisations and improvements could be made if spent tuning the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "get_mappings_func = generative_models.FunctionDeclaration(\n",
    "  name=\"get_mappings\",\n",
    "  description=\"Get the mappings of source schema fields for a given target schema field. This function provides the mappings as a confidence level for each source field to the target field. A single response from this function includes mappings for each and every source field that is presented. Each seperate source field should be recorded as matching indexes in the arrays for Source_Column_Names, Source_TableNames and Source_SchemaNames, where the same index of each array describes the source field details for that field. The response MUST have array lengths for Source_Column_Names, Source_TableNames, Source_SchemaNames that are equal to the number of source fields that are shown for a mapping request. for example the values in Source_Column_Names[0] Source_TableNames[0] Source_SchemaNames[0] together describe the attributes of the first source field, with Confidence_Levels[0] being the the confidence level out of 10 for mapping this source field to the target field. Simiarly for index [1] for the second field, index [2] for the third field, and so on for each source field.\",\n",
    "  parameters={\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "          \"Source_SchemaNames\": {\n",
    "              \"type\": \"array\",\n",
    "              \"description\": \"The array of values for the field SchemaNames in the source schema that could map to the target schema with a likelihood based on the corresponding value of Confidence_Level property at the same index.\",\n",
    "              \"items\" : {\n",
    "                    \"type\": \"string\"\n",
    "              },\n",
    "              \"example\": [\"dbo\",\"dbo\",\"dbo\"]\n",
    "          },\n",
    "          \"Source_TableNames\": {\n",
    "              \"type\": \"array\",\n",
    "              \"description\": \"The array of values for the field TableNames in the source schema that could map to the target schema with a likelihood based on the corresponding value of Confidence_Level property at the same index.\",\n",
    "              \"items\" : {\n",
    "                    \"type\": \"string\"\n",
    "              },\n",
    "              \"example\": [\"user_message\",\"CDLPolicyImportWrk\",\"clinic\"]\n",
    "          },\n",
    "          \"Source_Column_Names\": {\n",
    "              \"type\": \"array\",\n",
    "              \"description\": \"The array of values for the field Column_Names in the source schema that could map to the target schema with a likelihood based on the corresponding value of Confidence_Level property at the same index.\",\n",
    "              \"items\" : {\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"example\": [\"message_id\",\"PremiumPrice\",\"area_id\"]\n",
    "          },\n",
    "          \"Confidence_Levels\": {\n",
    "              \"type\": \"array\",\n",
    "              \"description\": \"The array of values for the confidence level out of 10 for the mappings of the source schema fields at the corresponding index. For example the array of ['3','8','2'] would mean a confidence level of 3/10 for the source>target schema mapping for the source field represented by the values in the first indexes of the Source_Column_Names, Source_TableNames and Source_SchemaNames arrays, a confidence level of 8/10 for the the source>target schema mapping for the source field represented by the values in the second indexes of the Source_Column_Names, Source_TableNames and Source_SchemaNames arrays, and so on for each of the indexes of the arrays.\",\n",
    "              \"items\" : {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]\n",
    "              },\n",
    "              \"example\": [\"1\",\"2\",\"3\"]\n",
    "          }\n",
    "      },\n",
    "      \"required\": [\n",
    "          \"Source_SchemaNames\", \"Source_TableNames\", \"Source_Column_Names\", \"Confidence_Levels\"\n",
    "      ]\n",
    "  },\n",
    ")\n",
    "\n",
    "get_mappings_tool = generative_models.Tool(\n",
    "  function_declarations=[get_mappings_func]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions used post LLM-response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function_call(function_call):\n",
    "    \"\"\"Parses a FunctionCall object, adds a description, and returns a JSON-compatible dictionary.\n",
    "\n",
    "    Args:\n",
    "        function_call: The FunctionCall object to parse.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the function name, attributes, and description.\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\n",
    "        \"function_name\": function_call.name,\n",
    "        \"attributes\": {},\n",
    "    }\n",
    "    for key, value in function_call.args.items():\n",
    "        result[\"attributes\"][key] = value\n",
    "\n",
    "    return result\n",
    "\n",
    "def convert_dict_to_list_of_dicts(dict):\n",
    "    \"\"\"Converts a dictionary of lists and strings to a list of flat dictionaries.\n",
    "\n",
    "    Args:\n",
    "        data: The input dictionary containing lists and strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a  \n",
    "        combination of elements from the input lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_attribute_dicts = []\n",
    "    string_keys = []\n",
    "    list_keys = []\n",
    "\n",
    "    \n",
    "    for key, value in dict.items() :\n",
    "        if isinstance(value, str):\n",
    "            string_keys.append(key)\n",
    "        else:\n",
    "            list_keys.append(key)       \n",
    "    \n",
    "    for i in range (len(dict[list_keys[0]])):\n",
    "        new_dict = {}\n",
    "        \n",
    "        for key in list_keys:\n",
    "            new_dict[key] = dict[key][i]\n",
    "        for key in string_keys:\n",
    "            new_dict[key] = dict[key]\n",
    "\n",
    "        list_of_attribute_dicts.append(new_dict)\n",
    "\n",
    "    return list_of_attribute_dicts\n",
    "\n",
    "def create_df_from_target_row_df_and_list_of_dicts(list_of_attribute_dicts, test_target_df_row):\n",
    "    \"\"\"\n",
    "    Appends rows to a DataFrame, combining a base row with data from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        list_of_dicts: A list of dictionaries, each representing column values.\n",
    "        test_target_df_row: A DataFrame row containing base columns.\n",
    "\n",
    "    Returns:\n",
    "        The modified DataFrame with the newly appended rows.\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "     \n",
    "    for attribute_dict in list_of_attribute_dicts:\n",
    "        # Combine the base row with the current dictionary\n",
    "\n",
    "        df = test_target_df_row.copy()  # Make a copy \n",
    "\n",
    "        for key in attribute_dict.keys():\n",
    "            df[key] = attribute_dict[key]\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over all source fields groups (~500), for a single target field\n",
    "\n",
    "- First we select a single Target schema field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare single target field\n",
    "test_target_df_row = target_df.iloc[[44]]\n",
    "test_target_string_row = dataframe_to_string(test_target_df_row)\n",
    "test_target_df_row.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_source_string_groups = []\n",
    "errored_source_string_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_for_bq_upload = []\n",
    "\n",
    "for j, unmapped_source_string_group in enumerate(unmapped_source_string_groups):\n",
    "\n",
    "    field_count = unmapped_source_string_group.count('Row:')\n",
    "    \n",
    "    print(\"************************************\")\n",
    "    print(f\"Attempting source field group {j}/{len(unmapped_source_string_groups)} = {j/len(unmapped_source_string_groups):.0%} ...\")\n",
    "    print(f\"{field_count} source fields in group\")\n",
    "    \n",
    "    prompt = f\"\"\"You are Data Engineer working for an insurance company. As part of a data migration project you need to assist with mapping fields in a source data schema fields in a target data schema.\n",
    "    The source and desination schemas are both complex and nested.\n",
    "    You will be shown 1 field in the target schema and multiple fields in the source schema.\n",
    "    The mappings will not be exactly one to one: Instead of providing a one-to-one mapping for a single source schema to a single destiation schema, you will be asked to provide a confidence rating for how well you think each of the fields for the source schemas you see will map to the field for the target schema.\n",
    "\n",
    "    The field from the target schema is described here:\n",
    "    {test_target_string_row}\n",
    "\n",
    "    The fields taken from the source schema are described here:\n",
    "    {unmapped_source_string_group}\n",
    "\n",
    "    Based on what you can see, I want you to provide a confidence level for whether any of the fields in the source schema map to the field of the target schema.\n",
    "    The confidence level is a number between 0 and 10 where 0 is very unlikely and 10 is a very strong match.\n",
    "\n",
    "    Please use the get_mappings_tool to structure your response, where each seperate field of the target schema should be described in the same index of the arrays for the Confidence_Levels, Source_Column_Names, Source_TableNames and Source_SchemaNames properties.\n",
    "\n",
    "    You are being shown {field_count} seperate source fields, so you need to have an array length of exactly {field_count} for each of the Confidence_Levels, Source_Column_Names, Source_TableNames and Source_SchemaNames properties in the response from the get_mappings tool.\n",
    "\n",
    "    As a general example to help you:\n",
    "    assuming a field the target schema is:\n",
    "    Column Names: Unique_Ref, Tranche, Level_1, Level_2, Level_3, Level_4, Complex_Type, Attribute, Description, Mandatory__, Data_Type, Accepted_Values, Validation, Drop_Down_Metaval\n",
    "    Row: 382, POLICY, Policy, quotesHubLifestyleFactor, n/a, n/a, quotesHubLifestyleFactor, code, The code identifying the Lifestyle factor, Mandatory, string (30), None, None, <NA>\n",
    "\n",
    "    and the fields for the source schema to provide a confidence level mapping for are:\n",
    "    Column Names: SchemaName, TableName, Column_Name, Data_type, Max_Length, precision, scale, is_nullable\n",
    "    Row: dbo, user_message, message_id, int, 4, 10, 0, 0\n",
    "    Row: dbo, CDLPolicyImportWrk, PremiumPrice, money, 8, 19, 4\n",
    "    Row: dbo, clinic, area_id, int, 4, 10, 0\n",
    "\n",
    "    You should structure the response with the get_mappings tool as follows:\n",
    "    Confidence_Levels = [\"X\", \"Y\", \"Z\"]\n",
    "    Source_SchemaNames = [\"dbo\",\"dbo\",\"dbo\"]\n",
    "    Source_TableNames = [\"user_message\",\"CDLPolicyImportWrk\",\"clinic\"]\n",
    "    Source_Column_Names = [\"message_id\",\"PremiumPrice\",\"area_id\"]\n",
    "\n",
    "    Where X represents your confidence out of 10 for how the sorce field dbo.user_message.message_id maps to the target field POLICY.Policy.quotesHubLifestyleFactor.quotesHubLifestyleFactor\n",
    "    Where Y represents your confidence out of 10 for how the sorce field dbo.CDLPolicyImportWrk.PremiumPrice maps to the target field POLICY.Policy.quotesHubLifestyleFactor.quotesHubLifestyleFactor\n",
    "    Where Z represents your confidence out of 10 for how the sorce field dbo.clinic.area_id maps to the target field POLICY.Policy.quotesHubLifestyleFactor.quotesHubLifestyleFactor\n",
    "\n",
    "    IT IS VERY IMPORTANT THAT IF THE LENGTHS OF THE ARRAYS YOU CREATE FOR THE PARAMETERS Source_Column_Names, Source_TableNames, Source_SchemaNames AND Confidence_Levels ARE EXACTLY THE SAME AS THE NUMBER OF SOURCE FIELDS YOU ARE SHOWN.\n",
    "    YOU MUST CREATE EXACTLY ONE CONFIDENCE LEVEL MAPPING FROM EACH SOURCE FIELD TO THE TARGET FIELD, AND FOLLOW THE INSTRUCTIONS EXACTLY FOR HOW TO STRUCTURE THIS INFORMATION INTO THE get_mappings TOOL.\n",
    "\n",
    "    There is a strong chance that none of the fields provide to you will match well - remember I am only sending you a small portion of the target schema and there may be other fields that you haven't seen which map much better.\n",
    "    \"\"\"\n",
    "\n",
    "    model_response_test2 = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"temperature\": 0},\n",
    "        tools=[get_mappings_tool],\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        function_call_json = parse_function_call(model_response_test2.candidates[0].content.parts[0].function_call)\n",
    "        attributes_dict = function_call_json[\"attributes\"]\n",
    "        print(f\"Received mapping response from Gemini: {attributes_dict}\")\n",
    "        \n",
    "        con1 = len(attributes_dict['Confidence_Levels']) != len(attributes_dict['Source_Column_Names'])\n",
    "        con2 = len(attributes_dict['Source_Column_Names']) != len(attributes_dict['Source_TableNames'])\n",
    "        con3 = len(attributes_dict['Source_TableNames']) != len(attributes_dict['Source_SchemaNames'])\n",
    "        if (con1 or con2 or con3):\n",
    "            print(\"*****UNBALANCED RESPONSE FROM GEMINI!*****\")\n",
    "            raise Exception\n",
    "        \n",
    "        con4 = len(attributes_dict['Source_Column_Names']) != field_count\n",
    "        if con4:\n",
    "            print(\"*****FIELDS MAPPED BY GEMINI IS NOT EQUAL TO NUMBER OF INPUT FIELDS!*****\")\n",
    "            raise Exception\n",
    "\n",
    "        list_of_attribute_dicts = convert_dict_to_list_of_dicts(attributes_dict) #flattens the response from the LLM so we now have a list containing 1 dict per source field > destination field mapping\n",
    "\n",
    "        if (field_count != len(list_of_attribute_dicts)):\n",
    "            print(\"*****INCORRECT MAPPING*****\")\n",
    "            raise Exception\n",
    "\n",
    "        group_mapping_output = create_df_from_target_row_df_and_list_of_dicts(list_of_attribute_dicts,test_target_df_row)\n",
    "        df_list_for_bq_upload.append(group_mapping_output)\n",
    "        print(f\"Prepared the mapping as a df ready for upload to bigquery\")\n",
    "\n",
    "        mapped_source_string_groups.append(unmapped_source_string_group)\n",
    "\n",
    "    except Exception as error:\n",
    "        print(\"*******FAILED SOURCE GROUP*********\")\n",
    "        print(unmapped_source_string_group)\n",
    "        print(\"*******FAILED SOURCE GROUP*********\")\n",
    "        print(\"*******ERROR*********\")\n",
    "        print(error)\n",
    "        print(\"*******ERROR*********\")\n",
    "        print(\"*******GEMINI RESPONSE*********\")\n",
    "        print(attributes_dict)\n",
    "        print(\"*******GEMINI RESPONSE*********\")\n",
    "        \n",
    "        errored_info = {\n",
    "            'unmapped_source_string_group': unmapped_source_string_group,\n",
    "            'error': error,\n",
    "            'function_call_json': function_call_json\n",
    "        }\n",
    "        errored_source_string_groups.append(errored_info)\n",
    "    \n",
    "    print(f\"error count is currently {len(errored_source_string_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading group_mapping into bigquery...\")\n",
    "print(f\"df_list_for_bq_upload contains {len(df_list_for_bq_upload)} dataframes. Concatenating...\")\n",
    "df_for_bq_upload = pd.concat(df_list_for_bq_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_bq_upload['Confidence_Levels'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(\"mapped-test2-v2\")\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[],\n",
    "    write_disposition=\"WRITE_APPEND\",\n",
    "    schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df_for_bq_upload, table_ref, job_config=job_config)  \n",
    "job.result()  # Wait for job completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
