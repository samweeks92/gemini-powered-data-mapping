{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv venv\n",
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install \"google-cloud-bigquery>=3.17\"\n",
    "!pip3 install \"google-cloud-aiplatform>=1.38\"\n",
    "!pip3 install \"pandas>=2.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "from vertexai.preview import generative_models\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "import re\n",
    "import time\n",
    "from proto.marshal.collections import repeated\n",
    "from proto.marshal.collections import maps\n",
    "\n",
    "from flask import Flask, request\n",
    "\n",
    "project_id = \"ai-sandbox-sw\"\n",
    "dataset_id = \"mstudy\"\n",
    "\n",
    "raw_target_table = \"target2\"\n",
    "target_table = \"target2_ordered\"\n",
    "\n",
    "raw_source_tables = [\"source-uipetmis\",\"source-uispet\"]\n",
    "raw_source_tables_wildcard = 'source-*'\n",
    "source_table = \"source_ordered\"\n",
    "\n",
    "job_scheduler_bucket_name = f\"{project_id}-job-scheduler-bucket-test\"\n",
    "queued_jobs_bucket_name = f\"{project_id}-queued-jobs-bucket-test\"\n",
    "\n",
    "in_progress_jobs_bucket_name = f\"{project_id}-in_progress_jobs_bucket_test\"\n",
    "failed_jobs_bucket_name = f\"{project_id}-failed_jobs_bucket_test\"\n",
    "successful_jobs_bucket_name = f\"{project_id}-completed_jobs_bucket\"\n",
    "bq_upload_queue_bucket_name = f\"{project_id}-bq_upload_queue_bucket_test\"\n",
    "\n",
    "\n",
    "client = bigquery.Client(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_ref_and_create_new_table(project_id, dataset_id, raw_table, new_table, new_col, prefix):\n",
    "    \"\"\"Adds 'Source_Unique_Ref' column if missing, then creates a new BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        project_id: BigQuery project ID.\n",
    "        dataset_id: BigQuery dataset ID.\n",
    "        raw_tables: Dict containing raw table names\n",
    "        new_table: Desired name of finalised table\n",
    "        new_col: the name of the reference column for the table\n",
    "    \"\"\"\n",
    "\n",
    "    raw_query = f\"\"\"\n",
    "        SELECT *  \n",
    "        FROM `{project_id}.{dataset_id}.{raw_table}`\n",
    "    \"\"\"\n",
    "    raw_df = client.query(raw_query).to_dataframe()\n",
    "\n",
    "    # Rename columns with prefix\n",
    "    for col in raw_df.columns:\n",
    "        print(f\"col is {col}\")\n",
    "        if col != f\"Unique_Ref\":\n",
    "            print(f\"therefore renaming {col} to {prefix}_{col}\")\n",
    "            raw_df.rename(columns={col: f\"{prefix}_{col}\"}, inplace=True)\n",
    "        else:\n",
    "            print(f\"therefore renaming {col} to Original_{col}\")\n",
    "            raw_df.rename(columns={col: f\"Original_{col}\"}, inplace=True)\n",
    "\n",
    "    raw_df[new_col] = range(1, len(raw_df) + 1)\n",
    "    \n",
    "    new_table_id = f\"{project_id}.{dataset_id}.{new_table}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig()  \n",
    "    job = client.load_table_from_dataframe(raw_df, new_table_id, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    return job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source table 'source_ordered' exists.\n",
      "source_df length is 5411\n",
      "Target table target2_ordered exists.\n",
      "target_df length is 997\n"
     ]
    }
   ],
   "source": [
    "# Source table setup\n",
    "source_table_ref = client.dataset(dataset_id).table(source_table) \n",
    "try:\n",
    "    client.get_table(source_table_ref)  # Will raise NotFound if the table doesn't exist\n",
    "    print(\"Source table '{}' exists.\".format(source_table))\n",
    "except:\n",
    "    print(\"Source table '{}' does not exist.\".format(source_table))\n",
    "    print(\"creating Source table...\")\n",
    "    new_source_col = 'Source_Unique_Ref'\n",
    "    source_prefix = \"Source\"   \n",
    "    add_unique_ref_and_create_new_table(project_id, dataset_id, raw_source_tables_wildcard, source_table, new_source_col, source_prefix)\n",
    "\n",
    "source_query = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{project_id}.{dataset_id}.{source_table}`\n",
    "\"\"\"\n",
    "source_df = client.query(source_query).to_dataframe()\n",
    "print(f\"source_df length is {source_df.shape[0]}\")\n",
    "\n",
    "\n",
    "# Target table setup\n",
    "target_table_ref = client.dataset(dataset_id).table(target_table) \n",
    "try:\n",
    "    client.get_table(target_table_ref)  # Will raise NotFound if the table doesn't exist\n",
    "    print(f\"Target table {target_table} exists.\")\n",
    "except:\n",
    "    print(f\"Target table {target_table} does not exist.\")\n",
    "    print(\"creating Target table...\")\n",
    "    new_target_col = 'Target_Unique_Ref'\n",
    "    target_prefix = \"Target\"   \n",
    "    add_unique_ref_and_create_new_table(project_id, dataset_id, raw_target_table, target_table, new_target_col, target_prefix)\n",
    "\n",
    "target_query = f\"\"\"\n",
    "    SELECT *  \n",
    "    FROM `{project_id}.{dataset_id}.{target_table}`\n",
    "\"\"\"\n",
    "target_df = client.query(target_query).to_dataframe()\n",
    "print(f\"target_df length is {target_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source_SchemaName</th>\n",
       "      <th>Source_TableName</th>\n",
       "      <th>Source_Column_Name</th>\n",
       "      <th>Source_Data_type</th>\n",
       "      <th>Source_Max_Length</th>\n",
       "      <th>Source_precision</th>\n",
       "      <th>Source_scale</th>\n",
       "      <th>Source_is_nullable</th>\n",
       "      <th>Source_Unique_Ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aggregator</td>\n",
       "      <td>coinsurance_override</td>\n",
       "      <td>force_coinsurance</td>\n",
       "      <td>bit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbo</td>\n",
       "      <td>bad_quotes_adu_20160722</td>\n",
       "      <td>confirm_excluded_breeds</td>\n",
       "      <td>bit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbo</td>\n",
       "      <td>blog</td>\n",
       "      <td>show</td>\n",
       "      <td>bit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbo</td>\n",
       "      <td>charity</td>\n",
       "      <td>active</td>\n",
       "      <td>bit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbo</td>\n",
       "      <td>discount</td>\n",
       "      <td>staff</td>\n",
       "      <td>bit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source_SchemaName         Source_TableName       Source_Column_Name  \\\n",
       "0        Aggregator     coinsurance_override        force_coinsurance   \n",
       "1               dbo  bad_quotes_adu_20160722  confirm_excluded_breeds   \n",
       "2               dbo                     blog                     show   \n",
       "3               dbo                  charity                   active   \n",
       "4               dbo                 discount                    staff   \n",
       "\n",
       "  Source_Data_type  Source_Max_Length  Source_precision  Source_scale  \\\n",
       "0              bit                  1                 1             0   \n",
       "1              bit                  1                 1             0   \n",
       "2              bit                  1                 1             0   \n",
       "3              bit                  1                 1             0   \n",
       "4              bit                  1                 1             0   \n",
       "\n",
       "   Source_is_nullable  Source_Unique_Ref  \n",
       "0                   0                  1  \n",
       "1                   0                  2  \n",
       "2                   0                  3  \n",
       "3                   0                  4  \n",
       "4                   0                  5  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Unique_Ref</th>\n",
       "      <th>Target_Tranche</th>\n",
       "      <th>Target_Level_1</th>\n",
       "      <th>Target_Level_2</th>\n",
       "      <th>Target_Level_3</th>\n",
       "      <th>Target_Level_4</th>\n",
       "      <th>Target_Complex_Type</th>\n",
       "      <th>Target_Attribute</th>\n",
       "      <th>Target_Description</th>\n",
       "      <th>Target_Mandatory__</th>\n",
       "      <th>Target_Data_Type</th>\n",
       "      <th>Target_Accepted_Values</th>\n",
       "      <th>Target_Validation</th>\n",
       "      <th>Target_Drop_Down_Metaval</th>\n",
       "      <th>Target_Unique_Ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CLIENT</td>\n",
       "      <td>Configuration</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>configuration</td>\n",
       "      <td>dataImportOperatorName</td>\n",
       "      <td>This is the logon name for the data import ope...</td>\n",
       "      <td>Mandatory</td>\n",
       "      <td>string</td>\n",
       "      <td>Reference Data: operator.loginName</td>\n",
       "      <td>Must be Strata Operator ID. ‘-11’</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CLIENT</td>\n",
       "      <td>Configuration</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>configuration</td>\n",
       "      <td>requestUUID</td>\n",
       "      <td>The is a unique identifier for the XML message</td>\n",
       "      <td>Optional</td>\n",
       "      <td>string (8)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CLIENT</td>\n",
       "      <td>Configuration</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>configuration</td>\n",
       "      <td>noUpdateMode</td>\n",
       "      <td>Setting to true will mean the XML message will...</td>\n",
       "      <td>Optional</td>\n",
       "      <td>boolean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CLIENT</td>\n",
       "      <td>Configuration</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>configuration</td>\n",
       "      <td>overrideStrataReferences</td>\n",
       "      <td>Setting to true will mean the classicOffsetRef...</td>\n",
       "      <td>Optional</td>\n",
       "      <td>integer</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>CLIENT</td>\n",
       "      <td>Configuration</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>configuration</td>\n",
       "      <td>useDvlaLookup</td>\n",
       "      <td>Specifies whether the DVLA service will be cal...</td>\n",
       "      <td>Optional</td>\n",
       "      <td>boolean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original_Unique_Ref Target_Tranche Target_Level_1 Target_Level_2  \\\n",
       "0                    1         CLIENT  Configuration            n/a   \n",
       "1                    2         CLIENT  Configuration            n/a   \n",
       "2                    3         CLIENT  Configuration            n/a   \n",
       "3                    4         CLIENT  Configuration            n/a   \n",
       "4                    5         CLIENT  Configuration            n/a   \n",
       "\n",
       "  Target_Level_3 Target_Level_4 Target_Complex_Type          Target_Attribute  \\\n",
       "0            n/a            n/a       configuration    dataImportOperatorName   \n",
       "1            n/a            n/a       configuration               requestUUID   \n",
       "2            n/a            n/a       configuration              noUpdateMode   \n",
       "3            n/a            n/a       configuration  overrideStrataReferences   \n",
       "4            n/a            n/a       configuration             useDvlaLookup   \n",
       "\n",
       "                                  Target_Description Target_Mandatory__  \\\n",
       "0  This is the logon name for the data import ope...          Mandatory   \n",
       "1     The is a unique identifier for the XML message           Optional   \n",
       "2  Setting to true will mean the XML message will...           Optional   \n",
       "3  Setting to true will mean the classicOffsetRef...           Optional   \n",
       "4  Specifies whether the DVLA service will be cal...           Optional   \n",
       "\n",
       "  Target_Data_Type              Target_Accepted_Values  \\\n",
       "0           string  Reference Data: operator.loginName   \n",
       "1       string (8)                                None   \n",
       "2          boolean                                None   \n",
       "3          integer                                None   \n",
       "4          boolean                                None   \n",
       "\n",
       "                   Target_Validation  Target_Drop_Down_Metaval  \\\n",
       "0  Must be Strata Operator ID. ‘-11’                      <NA>   \n",
       "1                               None                      <NA>   \n",
       "2                               None                      <NA>   \n",
       "3                               None                      <NA>   \n",
       "4                               None                      <NA>   \n",
       "\n",
       "   Target_Unique_Ref  \n",
       "0                  1  \n",
       "1                  2  \n",
       "2                  3  \n",
       "3                  4  \n",
       "4                  5  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_string(df):\n",
    "    \"\"\"Converts a DataFrame to a string with column names and row values.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to convert.\n",
    "\n",
    "    Returns:\n",
    "        A string representation of the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    output = f\"Column Names: {', '.join(df.columns)}\\n\"  # Header with column names\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_string = ', '.join(str(value) for value in row)\n",
    "        output += f\"Row: {row_string}\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_function_call(function_call):\n",
    "    \"\"\"Parses a FunctionCall object, adds a description, and returns a JSON-compatible dictionary.\n",
    "\n",
    "    Args:\n",
    "        function_call: The FunctionCall object to parse.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the function name, attributes, and description.\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\n",
    "        \"function_name\": function_call.name,\n",
    "        \"attributes\": {},\n",
    "    }\n",
    "    for key, value in function_call.args.items():\n",
    "        result[\"attributes\"][key] = value\n",
    "\n",
    "    return result\n",
    "\n",
    "def convert_dict_to_list_of_dicts(dict):\n",
    "    \"\"\"Converts a dictionary of lists and strings to a list of flat dictionaries.\n",
    "\n",
    "    Args:\n",
    "        data: The input dictionary containing lists and strings.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a  \n",
    "        combination of elements from the input lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_attribute_dicts = []\n",
    "    string_keys = []\n",
    "    list_keys = []\n",
    "\n",
    "    for key, value in dict.items() :\n",
    "        if isinstance(value, str):\n",
    "            string_keys.append(key)\n",
    "        else:\n",
    "            list_keys.append(key)       \n",
    "    \n",
    "    for i in range (len(dict[list_keys[0]])):\n",
    "        new_dict = {}\n",
    "        \n",
    "        for key in list_keys:\n",
    "            new_dict[key] = dict[key][i]\n",
    "        for key in string_keys:\n",
    "            new_dict[key] = dict[key]\n",
    "\n",
    "        list_of_attribute_dicts.append(new_dict)\n",
    "\n",
    "    return list_of_attribute_dicts\n",
    "\n",
    "def create_df_from_target_row_df_and_list_of_dicts(list_of_attribute_dicts, test_target_df_row):\n",
    "    \"\"\"\n",
    "    Appends rows to a DataFrame, combining a base row with data from a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        list_of_dicts: A list of dictionaries, each representing column values.\n",
    "        test_target_df_row: A DataFrame row containing base columns.\n",
    "\n",
    "    Returns:\n",
    "        The modified DataFrame with the newly appended rows.\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "     \n",
    "    for attribute_dict in list_of_attribute_dicts:\n",
    "        df = test_target_df_row.copy()  # Make a copy \n",
    "        for key in attribute_dict.keys(): # Combine the base row with the current dictionary\n",
    "            df[key] = attribute_dict[key]\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def merge_dataframes_and_string(target_df_row, source_df_row, confidence_level):\n",
    "    target_df_row = target_df_row.reset_index(drop=True)\n",
    "    source_df_row = source_df_row.reset_index(drop=True)\n",
    "    merged_df = pd.concat([target_df_row, source_df_row], axis=1)\n",
    "    confidence_df = pd.DataFrame({'Confidence_Levels': [confidence_level]})\n",
    "    final_df = pd.concat([merged_df, confidence_df], axis=1)\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_custom_target_string(df):\n",
    "\n",
    "    output = \"\"  # Header with column names\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_string = f\"target_field: {row['Target_Level_1']}\"\n",
    "        if row['Target_Level_2'] != 'n/a':\n",
    "            row_string += f\".{row['Target_Level_2']}\"\n",
    "        if row['Target_Level_3'] != 'n/a':\n",
    "            row_string += f\".{row['Target_Level_3']}\"\n",
    "        if row['Target_Level_4'] != 'n/a':\n",
    "            row_string += f\".{row['Target_Level_4']}\"\n",
    "        row_string += f\".{row['Target_Attribute']}; \"\n",
    "        if row['Target_Data_Type'] != '' and row['Target_Data_Type']:\n",
    "            row_string += f\"data_type: {row['Target_Data_Type']}; \"\n",
    "        if row['Target_Description'] != '' and row['Target_Description']:\n",
    "            row_string += f\"target_field_description: {row['Target_Description']}; \"\n",
    "        row_string += f\"target_field_unique_ref: {row['Target_Unique_Ref']}\"\n",
    "\n",
    "        output += f\"{row_string}\\n\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_row 86 source_group_start 52 source_group_end 104\n",
      "\n",
      "target_string_row\n",
      "target_field: Policy.ncbIndicator; data_type: boolean; target_field_description: Flag to indicate presence of previous insurance details; target_field_unique_ref: 125\n",
      "\n",
      "\n",
      "unmapped_source_string_groups[0]\n",
      "source_field: dbo.error_tracker_archive.notification_sent; data_type: bit; source_field_unique_ref: 788\n",
      "source_field: dbo.error_tracker_archive.id; data_type: int; source_field_unique_ref: 100\n",
      "source_field: dbo.error_tracker_archive.product_id; data_type: int; source_field_unique_ref: 923\n",
      "source_field: dbo.error_tracker_archive.error_message; data_type: varchar; source_field_unique_ref: 567\n",
      "source_field: dbo.error_tracker_archive.stack_trace; data_type: varchar; source_field_unique_ref: 1390\n",
      "source_field: dbo.error_tracker_archive.error_type; data_type: varchar; source_field_unique_ref: 1389\n",
      "source_field: dbo.error_tracker_archive.error_page; data_type: varchar; source_field_unique_ref: 1388\n",
      "source_field: dbo.error_tracker_archive.time_stamp; data_type: datetime; source_field_unique_ref: 678\n",
      "source_field: dbo.lead_failure_reason.active; data_type: bit; source_field_unique_ref: 789\n",
      "source_field: dbo.lead_failure_reason.id; data_type: int; source_field_unique_ref: 126\n",
      "source_field: dbo.lead_failure_reason.description; data_type: nvarchar; source_field_unique_ref: 696\n"
     ]
    }
   ],
   "source": [
    "objectId = \"target-row-86-source-groups-52-104\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "queued_jobs_bucket = storage_client.bucket(queued_jobs_bucket_name)\n",
    "failed_jobs_bucket = storage_client.bucket(failed_jobs_bucket_name)\n",
    "blob = queued_jobs_bucket.blob(objectId)\n",
    "\n",
    "if not blob.exists():\n",
    "    print(f\"File '{objectId}' not found in bucket '{queued_jobs_bucket_name}'. Job already picked from queue. Container instance completing with 204 message\")\n",
    "\n",
    "pattern = r\"^target-row-(\\d{1,4})-source-groups-(\\d{1,4})-(\\d{1,4})$\"\n",
    "match = re.match(pattern, objectId)\n",
    "if not match:\n",
    "    msg = \"objectId is not in the expected format: ^target-row-(\\d{3})-source-groups-(\\d{3})-(\\d{3})$\"\n",
    "    print(msg)\n",
    "\n",
    "target_row = int(match.group(1))\n",
    "source_group_start = int(match.group(2))\n",
    "source_group_end = int(match.group(3))\n",
    "\n",
    "print(f\"target_row {target_row} source_group_start {source_group_start} source_group_end {source_group_end}\")\n",
    "\n",
    "# Prepare target field\n",
    "target_df_row = target_df.iloc[[target_row]]\n",
    "target_string_row = dataframe_to_custom_target_string(target_df_row)\n",
    "print(\"\\ntarget_string_row\")\n",
    "print(target_string_row)\n",
    "\n",
    "\n",
    "contents = blob.download_as_string().decode('utf-8')\n",
    "unmapped_source_string_groups = []\n",
    "unmapped_source_string_groups = contents.split(\"\\n\\n\")  # Split by double newlines\n",
    "\n",
    "print(\"\\nunmapped_source_string_groups[0]\")\n",
    "print(unmapped_source_string_groups[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df_row['Target_Level_1'].iloc[0] == 'Policy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "set_source_field_mapping_confidence_levels = generative_models.FunctionDeclaration(\n",
    "    name=\"set_source_field_mapping_confidence_levels\",\n",
    "    description=\"\"\"Sets the mapping confidence values for each source field for a given target field.\n",
    "\n",
    "Here is a general example to help you understand how to use the set_source_field_mapping_confidences_tool correctly. This is only an example to show the source and target field structures.:\n",
    "\n",
    "Assuming you had previously decided on the following mapping confidence levels (but it is important that you come up with your own values for mapping condifence level rather than specifically using these values):\n",
    "a mapping confidence level of 2 for the field with source_field_unique_ref=158\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=159\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=1290\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=579\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=638\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=970\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=3317\n",
    "a mapping confidence level of 3 for the field with source_field_unique_ref=160\n",
    "a mapping confidence level of 1 for the field with source_field_unique_ref=1910\n",
    "a mapping confidence level of 5 for the field with source_field_unique_ref=2280\n",
    "\n",
    "Then this function would be used to set the mapping confidence levels for each of the source fields, where your input parameter source_field_mapping_confidences would be:\n",
    "source_field_mapping_confidences = [\n",
    "    {'source_field_unique_ref':158,'mapping_confidence_level':'2'},\n",
    "    {'source_field_unique_ref':159,'mapping_confidence_level':'2'},\n",
    "    {'source_field_unique_ref':1290,'mapping_confidence_level':'1'},\n",
    "    {'source_field_unique_ref':579,'mapping_confidence_level':'1'},\n",
    "    {'source_field_unique_ref':638,'mapping_confidence_level':'1'},\n",
    "    {'source_field_unique_ref':970,'mapping_confidence_level':'1'},\n",
    "    {'source_field_unique_ref':3317,'mapping_confidence_level':'1'},\n",
    "    {'source_field_unique_ref':160,'mapping_confidence_level':'3'},\n",
    "    {'source_field_unique_ref':1910,'mapping_confidence_level':'1'},\n",
    "    {'source_field_unique_ref':2280,'mapping_confidence_level':'5'}\n",
    "]\"\"\",\n",
    "\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"source_field_mapping_confidences\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"A List of objects where each object in the list contains the source field's source_field_unique_ref and the mapping_confidence_level for that source field.\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"source_field_unique_ref\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The reference ID for the source field.\"\n",
    "                        },\n",
    "                        \"mapping_confidence_level\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\n",
    "                            \"description\": \"The confidence level for the mapping (an integer between 1 and 5).\"\n",
    "                        },\n",
    "                        \"mapping_confidence_level_reason\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The reason why the source field should have this mapping confidence level value\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"source_field_unique_ref\", \"mapping_confidence_level\"]\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"source_field_mapping_confidences\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "set_source_field_mapping_confidence_levels_tool = generative_models.Tool(\n",
    "    function_declarations=[set_source_field_mapping_confidence_levels]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "11 source fields in group\n",
      "source_field: dbo.error_tracker_archive.notification_sent; data_type: bit; source_field_unique_ref: 788\n",
      "source_field: dbo.error_tracker_archive.id; data_type: int; source_field_unique_ref: 100\n",
      "source_field: dbo.error_tracker_archive.product_id; data_type: int; source_field_unique_ref: 923\n",
      "source_field: dbo.error_tracker_archive.error_message; data_type: varchar; source_field_unique_ref: 567\n",
      "source_field: dbo.error_tracker_archive.stack_trace; data_type: varchar; source_field_unique_ref: 1390\n",
      "source_field: dbo.error_tracker_archive.error_type; data_type: varchar; source_field_unique_ref: 1389\n",
      "source_field: dbo.error_tracker_archive.error_page; data_type: varchar; source_field_unique_ref: 1388\n",
      "source_field: dbo.error_tracker_archive.time_stamp; data_type: datetime; source_field_unique_ref: 678\n",
      "source_field: dbo.lead_failure_reason.active; data_type: bit; source_field_unique_ref: 789\n",
      "source_field: dbo.lead_failure_reason.id; data_type: int; source_field_unique_ref: 126\n",
      "source_field: dbo.lead_failure_reason.description; data_type: nvarchar; source_field_unique_ref: 696\n",
      "mapping to\n",
      "target_field: Policy.ncbIndicator; data_type: boolean; target_field_description: Flag to indicate presence of previous insurance details; target_field_unique_ref: 125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unmapped_source_string_group = unmapped_source_string_groups[0]\n",
    "field_count = unmapped_source_string_group.count('source_field:')\n",
    "\n",
    "print(\"************************************\")\n",
    "print(f\"{field_count} source fields in group\")\n",
    "print(unmapped_source_string_group)\n",
    "print(\"mapping to\")\n",
    "print(target_string_row)\n",
    "\n",
    "prompt = f\"\"\"You are Data Engineer working for an insurance company. As part of a data migration project you need to assist with mapping fields in a source data schema fields in a target data schema. Your job is to provide a mapping confidence level for how well you think each of the fields for the source schemas you see will map to the field for the target schema.\n",
    "This will be used as part of an automated data migration process so your mapping confidence level describes how confident you are that the data in the source field could be directly loaded into this target field without modification and it would make logical sense and contextual sense for the data to be put into that target field.\n",
    "\n",
    "Here is some information about the source fields:\n",
    "The fields from the source schema are also custom complex nested objects. They will have two levels of nesting, for example: Contact.Preference.Method\n",
    "Similar to the target field, each layer of nesting of the source fields is an important consideration for whether these source fields will map well to the target field.\n",
    "Similar to the target field, you may also be given the data types of the source fields. These are the standard types (e.g. string, int, boolean, dateTime, etc.) of the lowest level of nesting for each source field. These are also an important consideration for the mapping.\n",
    "\n",
    "Here is some information about the target field:\n",
    "The field from the target schema is a custom complex nested object. It will be at a minimum one level of nesting, for example {target_df_row['Target_Level_1'].iloc[0]}.id, and at a maximum 4 layers of nesting, for example {target_df_row['Target_Level_1'].iloc[0]}.namedPerson.namedDriver.email.emailAddress.\n",
    "Each layer of nesting is a vary important consideration for whether the source fields will map well to this target field, for example, consider the target fields Client.person.dateOfBirth and Policy.namedPerson.namedPerson (Motor).dateOfBirth. Although these have the same value for their lowest level of nesting in the field type (dateOfBirth), as first is referencing the date of birth of the client of the policy because its top level object is Client, and the second is referring to a named driver thats been added to a motor policy (which is not necessarily the same person), because its top level object is Policy. This example is to show you that its VERY important that you consider the ENTIRE nested structure of the target field to decide on the mapping confidence level.\n",
    "In addition, you may also be given the data type of the target field. This is the standard type (e.g. string, int, boolean, dateTime, etc.) of the lowest level of nesting for the field. This is also an important consideration for the mapping.\n",
    "In addition, there may also be a description of the target field. This is not present in all cases, but if it is present then please use this to help you better understand what this target field is referring to.\n",
    "You may also get additional information such as the possible accepted values for this target field, any validation logic for this field, or other information. Please use this information when making your mapping decision. \n",
    "\n",
    "You are being shown shown multiple fields in the source schema which are here:\n",
    "{unmapped_source_string_group}\n",
    "\n",
    "And one field from the target schema which is here:\n",
    "{target_string_row}\n",
    "\n",
    "Here is some information about how you should go about this job:\n",
    "Based on your knowledge of the insurance industry, home insurance, motor insurance, pets, pet insurance, and other related insurance industry concepts and data structures, you will provide a mapping confidence level for each of the source fields that describes how well you think that source field would map to the target field.\n",
    "You must think very carefully about the mapping confidence level you apply for each source field, as it will be used in later process steps to implement the automated data migration pipelines, so any inaccuracies lead to very costly errors.\n",
    "Remember that you need to consider EVERY NESTED LAYER of both the target field and the source fields to comprehend what kind of information they each represent, and therefore whether they are a good or bad mapping.\n",
    "As the value of the top level of the nested object of the target field is {target_df_row['Target_Level_1'].iloc[0]}, this means that the target field is only relevant to \n",
    "{\"the client that has taken out a policy.\" if target_df_row['Target_Level_1'].iloc[0] == 'Client' else \"\"}{\"a household insurance or home insurance policy.\" if target_df_row['Target_Level_1'].iloc[0] == 'Household Policy' else \"\"}{\"a motor insurance policy, or Motorcar policy data or car insurance data.\" if target_df_row['Target_Level_1'].iloc[0] == 'Motor Policy' else \"\"}{\"a pet insurance policy, or the pet that is being insured against in the pet insurance policy.\" if target_df_row['Target_Level_1'].iloc[0] == 'Pet' else \"\"}{\"a Policy that has been created or is being created by the client.\" if target_df_row['Target_Level_1'].iloc[0] == 'Policy' else \"\"}\n",
    "So you should only give high confidence mapping levels to source fields that also refer to {target_df_row['Target_Level_1'].iloc[0]} data. If you think the source field is not referring specifically to {target_df_row['Target_Level_1'].iloc[0]} data, you should not give a high mapping confidence level.\n",
    "You must also give a detailed reason for why you decided on that mapping confidence level.\n",
    "\n",
    "The mapping confidence level you will apply must be a number between 1 and 5 where:\n",
    "1 means there is a no chance that the source field matches the target field\n",
    "2 means there is a small chance the source field matches the target field\n",
    "3 means there is a good chance the source field matches the target field\n",
    "4 means there is a very good chance the source field matches the target field\n",
    "5 means there is a very very good chance the source field matches the target field\n",
    "\n",
    "You should decide on a mapping confidence level for each of the source fields, then set the mapping confidence level for each field using and use the value for source_field_unique_ref for each source field to reference it with its corresponding mapping confidence level as well as the reason for why you gave that confidence level.\n",
    "Then YOU MUST USE the available function set_source_field_mapping_confidence_levels in the set_source_field_mapping_confidence_levels_tool to set your mappings confidence level for each of the source fields.\n",
    "YOU MUST USE THIS FUNCTION.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[set_source_field_mapping_confidence_levels_tool],\n",
    ")\n",
    "\n",
    "if not model_response.candidates[0].content.parts[0].function_call:\n",
    "    print(\"did not use fn call! retrying with a more explicit prompt\")\n",
    "    prompt += \"\"\"\n",
    "YOU MUST USE THIS FUNCTION.\"\"\"\n",
    "    model_response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"temperature\": 0},\n",
    "        tools=[set_source_field_mapping_confidence_levels_tool],\n",
    "    )\n",
    "\n",
    "function_call_json = parse_function_call(model_response.candidates[0].content.parts[0].function_call)\n",
    "attributes_dict = function_call_json[\"attributes\"]\n",
    "print(f\"Received mapping response from Gemini: {attributes_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mapping_confidence_level': '1', 'source_field_unique_ref': 4689.0, 'mapping_confidence_level_reason': 'The data types of the source and target fields are different. The source field is smalldatetime and the target field is dateTime. This means that the source field may not be able to store all of the values that the target field can. Additionally, the source field is named applied_date, which suggests that it stores the date that a discount was applied, while the target field is named datePurchased, which suggests that it stores the date that a policy was purchased. These two concepts are not necessarily the same, so it is unlikely that the source field will map well to the target field.'}\n"
     ]
    }
   ],
   "source": [
    "def recurse_proto_repeated_composite(repeated_object):\n",
    "    repeated_list = []\n",
    "    for item in repeated_object:\n",
    "        if isinstance(item, repeated.RepeatedComposite):\n",
    "            item = recurse_proto_repeated_composite(item)\n",
    "            repeated_list.append(item)\n",
    "        elif isinstance(item, maps.MapComposite):\n",
    "            item = recurse_proto_marshal_to_dict(item)\n",
    "            repeated_list.append(item)\n",
    "        else:\n",
    "            repeated_list.append(item)\n",
    "\n",
    "    return repeated_list\n",
    "\n",
    "def recurse_proto_marshal_to_dict(marshal_object):\n",
    "    new_dict = {}\n",
    "    for k, v in marshal_object.items():\n",
    "      if not v:\n",
    "        continue\n",
    "      elif isinstance(v, maps.MapComposite):\n",
    "          v = recurse_proto_marshal_to_dict(v)\n",
    "      elif isinstance(v, repeated.RepeatedComposite):\n",
    "          v = recurse_proto_repeated_composite(v)\n",
    "      new_dict[k] = v\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "for attribute in attributes_dict['source_field_mapping_confidences']:\n",
    "    print(recurse_proto_marshal_to_dict(attribute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_for_bq_upload = []\n",
    "\n",
    "for l, mapping_confidence in enumerate(attributes_dict['source_field_mapping_confidences']):\n",
    "    mapping_confidence_dict = recurse_proto_marshal_to_dict(mapping_confidence)\n",
    "    Source_Unique_Ref = mapping_confidence_dict['source_field_unique_ref']\n",
    "    mapping_confidence_level_int = int(mapping_confidence_dict['mapping_confidence_level'])\n",
    "\n",
    "    source_df_row = source_df[source_df['Source_Unique_Ref']==Source_Unique_Ref]\n",
    "    \n",
    "    mapping_output = merge_dataframes_and_string(target_df_row, source_df_row,mapping_confidence_level_int)\n",
    "    df_list_for_bq_upload.append(mapping_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Unique_Ref</th>\n",
       "      <th>Target_Tranche</th>\n",
       "      <th>Target_Level_1</th>\n",
       "      <th>Target_Level_2</th>\n",
       "      <th>Target_Level_3</th>\n",
       "      <th>Target_Level_4</th>\n",
       "      <th>Target_Complex_Type</th>\n",
       "      <th>Target_Attribute</th>\n",
       "      <th>Target_Description</th>\n",
       "      <th>Target_Mandatory__</th>\n",
       "      <th>...</th>\n",
       "      <th>Source_SchemaName</th>\n",
       "      <th>Source_TableName</th>\n",
       "      <th>Source_Column_Name</th>\n",
       "      <th>Source_Data_type</th>\n",
       "      <th>Source_Max_Length</th>\n",
       "      <th>Source_precision</th>\n",
       "      <th>Source_scale</th>\n",
       "      <th>Source_is_nullable</th>\n",
       "      <th>Source_Unique_Ref</th>\n",
       "      <th>Confidence_Levels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>CLIENT</td>\n",
       "      <td>Client</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>n/a</td>\n",
       "      <td>client</td>\n",
       "      <td>clientExec</td>\n",
       "      <td>The Operator responsible for this client</td>\n",
       "      <td>Optional</td>\n",
       "      <td>...</td>\n",
       "      <td>dbo</td>\n",
       "      <td>error_tracker_archive</td>\n",
       "      <td>notification_sent</td>\n",
       "      <td>bit</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>788</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original_Unique_Ref Target_Tranche Target_Level_1 Target_Level_2  \\\n",
       "0                   12         CLIENT         Client            n/a   \n",
       "\n",
       "  Target_Level_3 Target_Level_4 Target_Complex_Type Target_Attribute  \\\n",
       "0            n/a            n/a              client       clientExec   \n",
       "\n",
       "                         Target_Description Target_Mandatory__  ...  \\\n",
       "0  The Operator responsible for this client           Optional  ...   \n",
       "\n",
       "  Source_SchemaName       Source_TableName Source_Column_Name  \\\n",
       "0               dbo  error_tracker_archive  notification_sent   \n",
       "\n",
       "   Source_Data_type  Source_Max_Length Source_precision Source_scale  \\\n",
       "0               bit                  1                1            0   \n",
       "\n",
       "  Source_is_nullable Source_Unique_Ref  Confidence_Levels  \n",
       "0                  1               788                  1  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list_for_bq_upload[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared the mapping as a df ready for upload to bigquery\n",
      "df_list_for_bq_upload contains 9 dataframes. Concatenating...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prepared the mapping as a df ready for upload to bigquery\")\n",
    "print(f\"df_list_for_bq_upload contains {len(df_list_for_bq_upload)} dataframes. Concatenating...\")\n",
    "df_for_bq_upload_concat = pd.concat(df_list_for_bq_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_bq_upload_concat.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_for_bq_upload_concat.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(dataset_id)\n",
    "table_ref = dataset_ref.table(\"mapped-test\")\n",
    "\n",
    "print(f\"Loading group_mapping into bigquery...\")\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[],\n",
    "    write_disposition=\"WRITE_APPEND\",\n",
    "    schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df_for_bq_upload_concat, table_ref, job_config=job_config)  \n",
    "job.result()  # Wait for job completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
